#include "ie_backend.h"
#include "utils.h"
#include <time.h>
//#include "spdlog/spdlog.h"
//#include "spdlog/sinks/basic_file_sink.h"


VINOInference::VINOInference(int model_index, std::string cpu_threads, bool force_cpu_mode, std::string model_folder)
{
	// logger
	//auto file_logger = spdlog::basic_logger_mt("ie_logger", "../logs/ie_logger.txt");
	//spdlog::set_default_logger(file_logger);
	Core ie;
	err_msg_ = "";
	predict_time_ = 0.;
	// ---------------------------- device selection --------------------------------------------------------
	std::vector<std::string> available_devices = ie.GetAvailableDevices();
	bool GPU_found = (std::find(available_devices.begin(), available_devices.end(), "GPU") != available_devices.end());
	bool gpu_mode = (!force_cpu_mode) & GPU_found;
	// ---------------------------- model selection --------------------------------------------------------
	if (gpu_mode)
	{
		model_folder = model_folder + "2"; // force to use the best model if gpu detected
	}
	else
	{
		model_folder = model_folder  + std::to_string(model_index);
	}
	// --------------------------- 1. Read IR Generated by ModelOptimizer (.xml and .bin files) ------------
	//auto model = kModelDir_ + '/' + std::to_string(input_shape_) + '/' + "model";
	auto model = getModelInFolder(model_folder);
	CNNNetReader networkReader;
	std::string model_xml = model[0];
	networkReader.ReadNetwork(model_xml);
	std::string bin_file_name = model[1];
	networkReader.ReadWeights(bin_file_name);
	CNNNetwork network = networkReader.getNetwork();
	network.setBatchSize(1);
	// --------------------------- shape inference ---------------------------------------------
	 //resize network if model's inputshapes not compatible with given shapes
	//"Shape Inference  within Inference Engine is tricky. It doesn't always work.  Smart Classroom Demo code shows how to use the Reshape API, namely :"
	auto input_shapes = network.getInputShapes();
	std::string input_name;
	SizeVector input_shape;
	std::tie(input_name, input_shape) = *input_shapes.begin(); 
	//if ((input_shape[2] != input_shape_) && (input_shape[3] != input_shape_))
	//{
	//	input_shape[0] = 1; //C
	//	input_shape[2] = input_shape_; //H
	//	input_shape[3] = input_shape_; //W
	//	input_shapes[input_name] = input_shape;
	//	network.reshape(input_shapes);
	//}
	// --------------------------- 2. Configure input & output ---------------------------------------------
	// --------------------------- Prepare input blobs -----------------------------------------------------
	input_shape_ = input_shape[2];
	InputsDataMap inputInfo(network.getInputsInfo());
	auto inputInfoItem = *inputInfo.begin(); // merely single input in the model
	input_name_ = inputInfoItem.first;
	auto inputData = inputInfoItem.second;
	inputData->setPrecision(Precision::U8);
	inputData->setLayout(Layout::NHWC);

	//inputData->getPreProcess().setColorFormat(ColorFormat::RGB);
	//inputData->getPreProcess().setResizeAlgorithm(RESIZE_BILINEAR); //preprocessing - resize
	// --------------------------- Prepare output blobs ----------------------------------------------------
	OutputsDataMap outputInfo(network.getOutputsInfo());
	auto outputInfoItem = *outputInfo.begin();
	output_name_ = outputInfoItem.first;
	auto outputData = outputInfoItem.second;
	outputData->setPrecision(Precision::FP32);
	outputData->setLayout(Layout::NCHW);
	// --------------------------- 3. initialize IECore && Loading model to specific device ------------------------------------------
	ExecutableNetwork executable_network;
	if (!gpu_mode)
	{
		std::map<std::string, std::string> ieConfig = {
			{InferenceEngine::PluginConfigParams::KEY_CPU_THREADS_NUM, cpu_threads},
			{InferenceEngine::PluginConfigParams::KEY_CPU_BIND_THREAD, InferenceEngine::PluginConfigParams::YES},
			{InferenceEngine::PluginConfigParams::KEY_CPU_THROUGHPUT_STREAMS, "1"},
			{InferenceEngine::PluginConfigParams::KEY_PERF_COUNT, "YES"}
		};
		auto extension_ptr = make_so_pointer<IExtension>("cpu_extension_avx2.dll");
		ie.AddExtension(extension_ptr, "CPU");
		executable_network = ie.LoadNetwork(network, "CPU", ieConfig);
	}
	else
	{
		std::map<std::string, std::string> ieConfig = {
			//{InferenceEngine::PluginConfigParams::KEY_SINGLE_THREAD, InferenceEngine::PluginConfigParams::YES},
			{InferenceEngine::PluginConfigParams::KEY_GPU_THROUGHPUT_STREAMS, "1"},
			{InferenceEngine::CLDNNConfigParams::KEY_CLDNN_PLUGIN_THROTTLE, "1"},
			{InferenceEngine::PluginConfigParams::KEY_PERF_COUNT, "YES"}
			//{InferenceEngine::PluginConfigParams::KEY_CPU_THROUGHPUT_STREAMS, "1"},
			//{InferenceEngine::PluginConfigParams::KEY_CPU_THREADS_NUM, "1"},

		};
		executable_network = ie.LoadNetwork(network, "GPU", ieConfig);
	}

	// --------------------------- 5. Create infer request -------------------------------------------------
	infer_request_ = executable_network.CreateInferRequest(); // the end of IE initialization

	//m_async_infer_request_curr_ = executable_network.CreateInferRequestPtr();
	//m_async_infer_request_next_ = executable_network.CreateInferRequestPtr();

		/* it's enough just to set image info input (if used in the model) only once */

	//}
	//catch (InferenceEngineException e) {
	//	//spdlog::error("error with OPENVINO_IE initialization");
	//	err_msg_ = "error with initializing openvino IE";
	//}
}



VINOInference::~VINOInference()
{
}



void VINOInference::getOutput(cv::Mat& matResult, InferRequest& inferRequest)
{
	//output_blob_ = infer_request_.GetBlob(output_name_);
	output_blob_ = inferRequest.GetBlob(output_name_);
	static auto output_data = output_blob_->buffer().as<float*>();

	size_t C, H, W;
	C = 2; // binary segmentation with 2 classes 
	H = input_shape_;
	W = input_shape_;
	size_t image_stride = W * H * 1;
	matResult = cv::Mat(cv::Size(W, H), CV_32FC1, output_data + image_stride);
	return ;
}


bool VINOInference::Predict(const cv::Mat& imageData, cv::Mat& result)
{
	img_blob_ = wrapMat2Blob(imageData);

	//img_blob_ = wrapMat2Blob(imageData);

	infer_request_.SetBlob(input_name_, img_blob_);
	infer_request_.Infer();

	getOutput(result, infer_request_);
	return true;
}

//bool VINOInference::PredictAsync(const cv::Mat& imageData, cv::Mat& result, bool restore_shape)
//{
//	static bool bFirst = true;
//	//static cv::Mat curr_frame;  
//	if (bFirst) {
//		bFirst = false;
//		//img_prepro_ = imageData;
//		return false;
//	}
//
//	//Preprocessing(curr_frame, img_prepro_);
//	img_blob_ = wrapMat2Blob(imageData);
//
//	m_async_infer_request_curr_->SetBlob(input_name_, img_blob_);
//	m_async_infer_request_curr_->StartAsync();
//
//	if (OK == m_async_infer_request_curr_->Wait(IInferRequest::WaitMode::RESULT_READY)) {
//		getOutput(result, m_async_infer_request_curr_);
//		if (restore_shape) RestoreShape(imageData, result); // restore to original image size
//	}
//	//curr_frame = imageData;
//	m_async_infer_request_curr_.swap(m_async_infer_request_next_);
//
//	return true;
//}


InferenceEngine::Blob::Ptr VINOInference::wrapMat2Blob(const cv::Mat& mat) {
	size_t channels = mat.channels();
	size_t height = mat.size().height;
	size_t width = mat.size().width;

	size_t strideH = mat.step.buf[0];
	size_t strideW = mat.step.buf[1];

	bool is_dense =
		strideW == channels &&
		strideH == channels * width;

	if (!is_dense) THROW_IE_EXCEPTION
		<< "Doesn't support conversion from not dense cv::Mat";

	InferenceEngine::TensorDesc tDesc(InferenceEngine::Precision::U8,
		{ 1, channels, height, width },
		InferenceEngine::Layout::NHWC);

	return InferenceEngine::make_shared_blob<uint8_t>(tDesc, mat.data);
}