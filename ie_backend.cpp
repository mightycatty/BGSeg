#include "ie_backend.h"
#include "utils.h"
#include <time.h>
//#include "spdlog/spdlog.h"
//#include "spdlog/sinks/basic_file_sink.h"


VINOInference::VINOInference(std::string device, std::string cpu_threads)
{
	// logger
	//auto file_logger = spdlog::basic_logger_mt("ie_logger", "../logs/ie_logger.txt");
	//spdlog::set_default_logger(file_logger);
	Core ie;
	err_msg_ = "";
	predict_time_ = 0.;
	// ---------------------------- device selection --------------------------------------------------------
	std::vector<std::string> available_devices = ie.GetAvailableDevices();
	bool GPU_found = (std::find(available_devices.begin(), available_devices.end(), "GPU") != available_devices.end());
	bool cpu_flag = (device == "CPU")? true : false;
	bool gpu_mode = (!cpu_flag) & GPU_found;
	std::string model_dir_name;
	if (gpu_mode)
	{
		model_dir_name = kModelSmall;
		input_width_ = 320;
	}
	else
	{
		model_dir_name = kModelSmall;
		input_width_ = 320;
	}
	// --------------------------- 1. Read IR Generated by ModelOptimizer (.xml and .bin files) ------------
	CNNNetReader networkReader;
	std::string model_xml = model_dir_name + ".xml";
	networkReader.ReadNetwork(model_xml);
	std::string bin_file_name = model_dir_name + ".bin";
	networkReader.ReadWeights(bin_file_name);
	CNNNetwork network = networkReader.getNetwork();
	network.setBatchSize(1);
	// --------------------------- 2. Configure input & output ---------------------------------------------
	// --------------------------- Prepare input blobs -----------------------------------------------------
	InputsDataMap inputInfo(network.getInputsInfo());
	auto inputInfoItem = *inputInfo.begin(); // merely single input in the model
	auto inputData = inputInfoItem.second;
	input_name_ = inputInfoItem.first;
	inputData->setPrecision(Precision::U8);
	inputData->getPreProcess().setColorFormat(ColorFormat::RGB);
	inputData->setLayout(Layout::NCHW);
	inputData->getPreProcess().setResizeAlgorithm(RESIZE_BILINEAR);
	// --------------------------- Prepare output blobs ----------------------------------------------------
	OutputsDataMap outputInfo(network.getOutputsInfo());
	auto outputInfoItem = *outputInfo.begin();
	output_name_ = outputInfoItem.first;
	auto outputData = outputInfoItem.second;
	outputData->setPrecision(Precision::FP32);
	outputData->setLayout(Layout::NCHW);
	// --------------------------- 3. initialize IECore && Loading model to specific device ------------------------------------------
	ExecutableNetwork executable_network;
	// TODO: device 
	if (!gpu_mode)
	{
		std::map<std::string, std::string> ieConfig = {
			{InferenceEngine::PluginConfigParams::KEY_CPU_THREADS_NUM, cpu_threads},
			{InferenceEngine::PluginConfigParams::KEY_CPU_BIND_THREAD, InferenceEngine::PluginConfigParams::YES},
			{InferenceEngine::PluginConfigParams::KEY_CPU_THROUGHPUT_STREAMS, "1"}
		};
		auto extension_ptr = make_so_pointer<IExtension>("cpu_extension_avx2.dll");
		ie.AddExtension(extension_ptr, "CPU");
		executable_network = ie.LoadNetwork(network, "CPU", ieConfig);
	}
	else
	{
		std::map<std::string, std::string> ieConfig = {
			//{InferenceEngine::PluginConfigParams::KEY_SINGLE_THREAD, InferenceEngine::PluginConfigParams::YES},
			{InferenceEngine::PluginConfigParams::KEY_GPU_THROUGHPUT_STREAMS, "1"},
			{InferenceEngine::CLDNNConfigParams::KEY_CLDNN_PLUGIN_THROTTLE, "1"},
			//{InferenceEngine::PluginConfigParams::KEY_CPU_THROUGHPUT_STREAMS, "1"},
			//{InferenceEngine::PluginConfigParams::KEY_CPU_THREADS_NUM, "1"},

		};
		executable_network = ie.LoadNetwork(network, "GPU", ieConfig);
	}

	// --------------------------- 5. Create infer request -------------------------------------------------
	infer_request_ = executable_network.CreateInferRequest(); // the end of IE initialization

	//m_async_infer_request_curr_ = executable_network.CreateInferRequestPtr();
	//m_async_infer_request_next_ = executable_network.CreateInferRequestPtr();

		/* it's enough just to set image info input (if used in the model) only once */

	//}
	//catch (InferenceEngineException e) {
	//	//spdlog::error("error with OPENVINO_IE initialization");
	//	err_msg_ = "error with initializing openvino IE";
	//}
}



VINOInference::~VINOInference()
{
}


// unneccesary when embeded in IE
void VINOInference::Preprocessing(const cv::Mat& kInputImg, cv::Mat& output_img, int resize_width)
{
	//ResizeWithPadding(kInputImg, output_img, resize_width);
	cv::resize(kInputImg, output_img, cv::Size(resize_width, resize_width));
	return ;
}


// trim padding and restore to the original shape
void VINOInference::RestoreShape(const cv::Mat& inputImg, cv::Mat& mask)
{
	int or_height = inputImg.size().height;
	int or_width = inputImg.size().width;
	resize(mask, mask, cv::Size(or_width, or_height));
}


void VINOInference::getOutput(cv::Mat& matResult, InferRequest& inferRequest)
{
	//output_blob_ = infer_request_.GetBlob(output_name_);
	output_blob_ = inferRequest.GetBlob(output_name_);
	static auto output_data = output_blob_->buffer().as<float*>();

	size_t C, H, W;
	C = 2; // binary segmentation with 2 classes 
	H = input_width_;
	W = input_width_;
	size_t image_stride = W * H * 1;
	///** Iterating over each pixel **/
	//matResult = cv::Mat(cv::Size(W, H), CV_8UC1);
	//matResult = cv::Mat(cv::Size(W, H), CV_32FC1);
	//for (size_t w = 0; w < W; ++w) {
	//	for (size_t h = 0; h < H; ++h) {
	//		/*if (output_data[W * h + w] > output_data[W * H * 1 + W * h + w])
	//		{
	//			matResult.at<uchar>(h, w) = 0;
	//		}
	//		else
	//		{
	//			matResult.at<uchar>(h, w) = 1;
	//		}*/
	//		matResult.at<float>(h, w) = output_data[W * H * 1 + W * h + w];
	//	}
	//}
	matResult = cv::Mat(cv::Size(W, H), CV_32FC1, output_data + image_stride);
	return ;
}


bool VINOInference::Predict(const cv::Mat& imageData, cv::Mat& result, bool restore_shape)
{
	img_blob_ = wrapMat2Blob(imageData);

	infer_request_.SetBlob(input_name_, img_blob_);
	infer_request_.Infer();

	getOutput(result, infer_request_);
	if (restore_shape) RestoreShape(result, result); // restore to original image size
	return true;
}

//bool VINOInference::PredictAsync(const cv::Mat& imageData, cv::Mat& result, bool restore_shape)
//{
//	static bool bFirst = true;
//	//static cv::Mat curr_frame;  
//	if (bFirst) {
//		bFirst = false;
//		//img_prepro_ = imageData;
//		return false;
//	}
//
//	//Preprocessing(curr_frame, img_prepro_);
//	img_blob_ = wrapMat2Blob(imageData);
//
//	m_async_infer_request_curr_->SetBlob(input_name_, img_blob_);
//	m_async_infer_request_curr_->StartAsync();
//
//	if (OK == m_async_infer_request_curr_->Wait(IInferRequest::WaitMode::RESULT_READY)) {
//		getOutput(result, m_async_infer_request_curr_);
//		if (restore_shape) RestoreShape(imageData, result); // restore to original image size
//	}
//	//curr_frame = imageData;
//	m_async_infer_request_curr_.swap(m_async_infer_request_next_);
//
//	return true;
//}


InferenceEngine::Blob::Ptr VINOInference::wrapMat2Blob(const cv::Mat& mat) {
	size_t channels = mat.channels();
	size_t height = mat.size().height;
	size_t width = mat.size().width;

	size_t strideH = mat.step.buf[0];
	size_t strideW = mat.step.buf[1];

	bool is_dense =
		strideW == channels &&
		strideH == channels * width;

	if (!is_dense) THROW_IE_EXCEPTION
		<< "Doesn't support conversion from not dense cv::Mat";

	InferenceEngine::TensorDesc tDesc(InferenceEngine::Precision::U8,
		{ 1, channels, height, width },
		InferenceEngine::Layout::NHWC);

	return InferenceEngine::make_shared_blob<uint8_t>(tDesc, mat.data);
}